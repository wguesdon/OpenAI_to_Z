Guide to High-Scoring Amazon Archaeological Site Discovery
Introduction
Identifying unknown archaeological sites in the vast Amazon biome is a complex but exciting challenge. Recent competitions (such as the “OpenAI to Z” challenge) task participants with finding previously unknown Amazonian archaeological sites using open-source data. Success requires a blend of archaeological savvy, remote sensing skills, and cutting-edge AI techniques – all while providing solid evidence, reproducible methods, and a novel contribution. This guide provides a comprehensive approach for a solo researcher with moderate AI/LLM experience but limited archaeology background. We cover how sites are discovered in dense forests, outline a state-of-the-art open-data methodology, suggest ways to integrate large language models (LLMs) at each step, highlight useful LLM toolchains (LangChain, LangGraph, etc.), and offer tips to maximize clarity, reproducibility, novelty, and presentation quality in your final submission.
1. How New Archaeological Sites Are Discovered in the Amazon
Discovering ancient sites beneath the Amazon rainforest canopy is notoriously difficult. Traditional on-foot exploration is slow in dense jungle, so archaeologists increasingly rely on technology and strategic clues. Here’s a step-by-step overview of typical discovery methods in Amazonia’s forested context:
Preliminary Research & Clue Gathering: Archaeologists often start by studying historical records, indigenous knowledge, and previous research. Old expedition diaries, colonial maps, or local legends might hint at lost settlements or earthworks. For example, 20th-century explorer Percy Fawcett’s search for a “Lost City of Z” was inspired by such accounts
smithsonianmag.com
. Modern researchers might comb through missionary chronicles or scientific reports for references to mounds, roads, or ancient villages.
Remote Sensing Surveys: Technological surveys dramatically increase the odds of finding sites hidden under forest cover:
Aerial and Satellite Imagery: High-resolution satellite images (e.g. from Google Earth, Sentinel-2, or Planet imagery) can reveal large geometric clearings or soil discolorations where vegetation has been disturbed by ancient human activity. In the southwestern Amazon (Acre, Brazil), dozens of massive geometric earthworks (“geoglyphs”) were first spotted on satellite photos after deforestation
jqjacobs.net
. Systematic scanning of updated Google Earth imagery between 2005 and 2010 rapidly raised the count of known Amazonian geoglyphs from 32 to about 270
jqjacobs.net
. Researchers leveraged free tools like Google Earth to virtually “prospect” these patterns, multiplying discoveries as new high-resolution images became available
jqjacobs.net
.
LiDAR (Light Detection and Ranging): LiDAR has been a game-changer for seeing through dense canopy. Mounted on aircraft or drones, LiDAR pulses create detailed 3D models of the ground by “digitally deforesting” the canopy. In 2022, scientists flying LiDAR over the Bolivian Amazon uncovered the ruins of a vast urban settlement of the Casarabe culture – including monumental platforms, pyramid mounds, and a network of causeways – all hidden under unbroken jungle
smithsonianmag.com
. The LiDAR-based digital terrain model revealed man-made geometric features (rectangular plazas, canals, concentric ditches, etc.) that were essentially invisible to satellites or the naked eye on the ground. (Image below: A LiDAR-derived 3D map showing a buried Amazonian settlement; green indicates lower areas and red higher elevations, highlighting a central platform mound and surrounding structures)_

smithsonianmag.com
. Such examples show how LiDAR can detect subtle elevation anomalies (like earth mounds or ditches) beneath vegetation.
Aerial Photography and SAR: Even before LiDAR, archaeologists used aerial photos (from planes or drones) to spot large features during dry seasons or in deforested patches. Synthetic Aperture Radar (SAR) imagery (e.g., from satellites like Sentinel-1) can penetrate some vegetation and reveal linear features or mounds under the canopy by detecting different moisture or texture, though its resolution is lower than optical or LiDAR. These methods can flag candidate sites for closer inspection.
Ground Truthing and Field Surveys: Remote sensing gives targets, but field verification is key. Archaeologists will organize expeditions to promising locations to confirm a site’s existence and nature. On the ground, they look for telltale signs: artificial mounds, ditches, pottery shards, dark earth (terra preta soil indicating past habitation), or remnants of structures. For instance, once satellite and LiDAR identified potential earthworks, teams visited those coordinates with GPS to verify geometric ditches or embankments and to collect archaeological samples (ceramics, charcoal for dating, etc.). Such field surveys ensure the remote observations are truly cultural (man-made) and not natural quirks. They also provide material evidence (artifacts, radiocarbon dates) to legitimize the discovery with hard data.
Documentation and Cross-Referencing: As new sites are found, they are cross-checked against existing site databases and literature. This is to ensure the site is indeed “previously unknown” (a novel discovery) and not a rediscovery of a known site under a new name. Researchers maintain inventories of known Amazonian sites (for example, published geoglyph catalogs
jqjacobs.net
jqjacobs.net
). Checking your discovery against such records (and even against coordinates mentioned in academic papers or local heritage registers) is crucial. This negative evidence step prevents duplicates and demonstrates due diligence. Only after confirming its novelty will archaeologists announce a new site.
Iterative Pattern Search: Discoveries often lead to more discoveries. Archaeologists analyze spatial patterns – where sites tend to occur – to guide further searches. In the Amazon, studies noted that ancient earthworks often cluster near water sources, on slightly elevated ground above seasonal floods, or along fertile zones. Once a few sites are found in a region, researchers examine similar environmental settings nearby to predict where other sites of the same culture might lie
jqjacobs.net
peerj.com
. This “from the known to the unknown” approach helps expand the search in a systematic way. It’s essentially a form of predictive modeling done traditionally by expert intuition, now increasingly enhanced by machine learning (discussed later). By looking for areas with matching characteristics – e.g. a hilltop near a river, or certain soil types – archaeologists increase their chances of spotting the subtle signs of an undiscovered site.
Collaboration with Local Communities: In many cases, indigenous and local communities are aware of unusual features (mounds, old pottery, legends of past settlements). Engaging with them can provide valuable leads. For example, villagers might know of “ancient fortifications” or curiously straight causeways in the forest. Such leads can direct remote sensing and surveys to the right vicinity. Conversely, new remote-sensed discoveries are often shared back with local authorities to aid in site protection (especially as many Amazon sites are threatened by deforestation and agriculture
jqjacobs.net
jqjacobs.net
).
By combining historical research, remote sensing (satellite, LiDAR, aerial), and ground validation, archaeologists have revealed thousands of pre-Columbian earthworks across Amazonia in recent decades. Your approach in the competition should mirror this multi-pronged strategy: use every available clue and tool (from colonial-era maps to modern AI) to pinpoint places where an ancient society might have left its mark.
2. Baseline Methodology (Open-Source & Reproducible) for a Solo Participant
A strong competition entry will demonstrate a state-of-the-art yet reproducible methodology to locate new sites. This means using publicly available data sources and well-documented analysis steps that others (including judges) can verify. Below is a baseline pipeline that a solo researcher could implement with open data and open-source tools:
2.1 Data Collection: Open-Source Datasets
Gathering the right data is the foundation. Fortunately, many relevant datasets for Amazon archaeology are free and open-access:
Satellite Imagery: High-resolution imagery is crucial for scanning large areas.
Sentinel-2 (Copernicus) – Multispectral optical images at 10–20 m resolution, updated every 5 days. These are available for free via the Copernicus Data Hub or the AWS Open Data registry
registry.opendata.aws
. Sentinel-2’s near-infrared bands are useful for assessing vegetation health (NDVI), which can sometimes reveal subtle ground disturbances (e.g. vegetation growing differently over ancient mounds or ditches).
Landsat 8/9 (USGS/NASA) – 30 m multispectral imagery (free via USGS EarthExplorer or Google Earth Engine). Landsat has a long historical record; older images might show sites when deforestation or seasonal droughts made them briefly visible.
Planet NICFI Tropical Mosaics – As part of a climate initiative, Planet Labs provides free monthly mosaics at ~4 m resolution for the tropics (including the Amazon) to non-commercial users. This could be accessed via Google Earth Engine or Planet’s API after obtaining a free license. These mosaics can be extremely useful for spotting small clearings or linear features in the forest with higher detail than Sentinel-2.
Synthetic Aperture Radar (SAR): Sentinel-1 SAR (10 m, C-band) data is openly available and can penetrate cloud cover and some canopy. While SAR won’t directly show ancient structures, it can detect differences in surface roughness or moisture that might correlate with archaeological features (e.g. raised berms might drain differently, showing contrast in radar backscatter). SAR imagery, combined with optical, could provide additional evidence especially in persistently cloudy areas.
Digital Globe/Maxar Imagery: These are very high resolution (~0.5 m) commercial satellites. They’re not fully open data, but platforms like Google Earth or Bing Maps offer viewing access. Many known geoglyphs in Amazon were actually discovered by enthusiasts scanning Google Earth’s high-res imagery
jqjacobs.net
. While you can’t bulk-download these, you can use Google Earth as a visual tool to inspect any suspect coordinates you find from coarser data.
Elevation and Lidar Data:
SRTM DEM (Shuttle Radar Topography Mission): A global 30 m Digital Elevation Model (DEM) available via NASA/USGS. SRTM gives ground elevation (with some tree height mixed in for forested areas)
daac.ornl.gov
. It’s coarse, but large earthworks like extensive mounds or ditches might subtly register in the DEM. Also, derivatives like slope or local relief models from SRTM can highlight unnatural flat-topped hills or geometric embankments.
NASA GEDI: The GEDI LiDAR mission (Global Ecosystem Dynamics Investigation) provides sample-based 3D measurements of forests. All GEDI data products are free to download
gedi.umd.edu
. GEDI won’t give a full image, but it offers waveforms or point measurements of canopy height and ground elevation at spots (approximately a 25 m footprint). If a GEDI shot passes over an archaeological earthwork, it might show an anomalous lower canopy or an elevated ground height compared to surrounding forest. For example, a broad ditch or plaza might appear as a dip in canopy height and an even dip in ground return. Incorporating GEDI data in your analysis can strengthen evidence (as hinted by challenge organizers) – e.g., seeing a “canopy dip” aligning with a LiDAR-detected structure
cdn.openai.com
cdn.openai.com
.
Airborne LiDAR Surveys: While full LiDAR coverage of the Amazon is not publicly available (surveys are sparse and often proprietary), some open samples exist. Check the Registry of Open Data on AWS or platforms like OpenTopography for any Amazon LiDAR datasets. For instance, small-area LiDAR scans near Manaus (Brazil) are published for research
daac.ornl.gov
, and the Brazilian government or NGOs sometimes release LiDAR data for specific archaeological studies. In absence of large-scale LiDAR, you might rely on processed products from literature (some papers provide detected features or anomaly maps). However, if you can obtain or derive any high-res elevation models, they could be the key to spotting hidden geometric patterns.
Digital Terrain Models (DTMs): If LiDAR point clouds are available, you’d process them to DTMs (ground models with vegetation removed). If not, one can attempt to approximate a DTM by subtracting canopy heights (from GEDI or other sources) from a surface model, but results are coarse. Specialized algorithms can then highlight anomalies in DTMs: e.g., a 2022 study by Wagner et al. introduced a fast method to compute “DTM anomalies” that automatically flag potential geoglyph shapes
cdn.openai.com
. A solo participant could implement simpler versions of this – for example, applying high-pass filters or edge detection on elevation grids to reveal rectangular or circular outlines.
Environmental & Ancillary Data: Ancient Amazonian people often chose sites based on environment. Collecting layers of environmental data can help predict or validate site locations:
Hydrography: Rivers, streams, and lakes (e.g., data from HydroSHEDS or Amazon basin shapefiles). Many settlements were near water (for transport and resources). Distance-to-water or river network maps can be generated and used to pinpoint well-drained but accessible spots.
Soil and Vegetation: Datasets like the Harmonized World Soil Database (which includes soil fertility and texture)
peerj.com
 or land cover maps can be relevant. Notably, Amazonian Dark Earth (“terra preta”) is a human-created fertile soil found at habitation sites; while there’s no direct map of terra preta, some soil property layers or even certain tree species distributions might act as proxies.
Climate and Topography: WorldClim climate data (temperature, precipitation averages)
peerj.com
 can characterize areas; for example, some cultures farmed on drier uplands vs. floodplains. Terrain derivatives (slope, elevation from DEM) help find elevated bluffs safe from flooding where people built earthworks.
Deforestation and Land Use: Knowing which areas are cleared vs forested today is important. Sites in cleared areas might be visible in imagery (but also at higher risk of destruction), whereas sites in intact forest likely need LiDAR to find. Data like global forest loss maps (e.g., Hansen et al.) or land cover maps (like ESA’s WorldCover) can inform your strategy – e.g., focus optical image analysis on already deforested patches first (where something might directly show up) and use predictive models for forested regions (to then prioritize for LiDAR or future exploration)
peerj.com
.
Historical & Cultural Data: For the archival research component, gather text and map data:
Exploration Accounts: Download public-domain books from sources like Internet Archive or the Library of Congress digital collections. Many 19th–early 20th century explorers in the Amazon kept detailed journals with notes of ancient mounds or roads
cdn.openai.com
. For example, the Library of Congress has river-mile diaries of Amazon expeditions that list indigenous village locations – these could hint at nearby archaeological remains
cdn.openai.com
. Having these texts in digital form is valuable for text-mining (where LLMs can assist, as we’ll see).
Known Site Catalogs: Compile coordinates of known archaeological sites to use as “training data” or at least to exclude from your predictions (to ensure novelty). Sources include published papers (some provide tables or supplemental datasets of site coords) – e.g., a 2024 study in Acta Amazonica mapped 1,279 earthwork sites in SW Amazonia
jqjacobs.net
 (with a dataset of coordinates
jqjacobs.net
), or the online repository by researcher J. Q. Jacobs which lists over 1,500 geoglyph and mound sites as of 2025
jqjacobs.net
. You can also use UNESCO or national heritage databases if available. Gathering these known sites helps you both learn the environmental signatures and avoid duplicating known discoveries.
2.2 Analysis Methodology: Finding Needles in the Green Hell
With data in hand, the next step is analysis. A robust methodology will likely combine predictive modeling with visual/algorithmic detection, plus verification steps:
Exploratory Data Analysis (EDA): Begin by examining the known sites and data layers. What do the known archaeological sites have in common environmentally? You might calculate statistics: e.g., “80% of known geoglyphs in Acre are within 5 km of a river” or “ancient villages often sit on soil with above-average fertility”. Look at satellite images of known sites to recognize patterns (e.g., many geoglyphs are on flat plateau tops with grassland). This EDA helps inform both manual searching and machine learning features.
Machine Learning Predictive Model: A state-of-the-art approach, demonstrated by recent research, is to use machine learning to predict site likelihood across the landscape
peerj.com
peerj.com
. In 2023, Walker et al. used over 1,100 known Amazonian sites (earthworks, terra preta sites, etc.) and 65 geospatial variables to train a model that outputs the probability of archaeological site presence
peerj.com
. They included climate, soil, elevation, and distances to water or ecotones as inputs, and found that a Random Forest classifier performed particularly well
peerj.com
peerj.com
. By including multiple site types in one model, they achieved good generalizability in predictions
peerj.com
. You can adopt a similar approach:
Prepare a grid over the Amazon or focus region (say 1 km or 5 km cells).
For each cell, compute features: distance to nearest river, elevation, avg rainfall, soil type indicators, etc. (Many of these can be extracted from the datasets above; e.g., use QGIS or Python (GeoPandas/Rasterio) to overlay point data and raster layers).
Mark grid cells as “site” if a known site falls inside, otherwise “non-site” (taking care to balance or downsample non-site samples, and perhaps exclude cells in heavily surveyed areas to avoid bias).
Train a classifier (Random Forest, XGBoost, or even a Neural Network) to distinguish site vs non-site based on the features. Ensure to use spatial cross-validation if possible (to avoid overfitting to clusters of known sites)
peerj.com
.
The result is a predictive map highlighting high-probability cells. This can guide you on where to look more closely. Important: treat the model output as suggestive rather than definitive. It narrows down search areas – perhaps flagging certain river basins or certain terraces as likely spots for undiscovered sites.
Remote Sensing Detection: In parallel with the ML model, perform direct analysis on imagery for visual evidence of sites:
For already deforested areas: Use Google Earth or GIS to scan for geometric shapes (circles, rectangles, straight lines) in cleared fields. Many Amazonian earthworks appear as dark outlines in soil or lighter regrowth in a geometric pattern when seen from above. For instance, (image below: Satellite view of a square-in-circle geoglyph enclosure near Rio Branco, Acre)_ shows how a prehistoric ditched enclosure appears as a square inside a circular outline in grassland

. This was identifiable once high-res satellite imagery became available, leading to dozens of such finds
jqjacobs.net
jqjacobs.net
. You can automate some of this via image processing: e.g., apply edge detectors or look for circularity in binary masks. But given the scale, manual scanning aided by Google Earth’s community placemarks (many known geoglyph locations are shared as KML files) can help ensure you’re not missing obvious ones. Mark any new shapes you find that aren’t in known site lists.
For forested areas: This is where LiDAR (if available) or clever use of proxies comes in. If you have any LiDAR-derived DTM tiles from public data or past studies, run them through a slope or relief visualization to spot unnatural shapes. Even without new LiDAR, examine terrain models like SRTM for slight flat-topped hills or linear rises that don’t match natural river levees. Multi-temporal satellite data might offer clues – for example, during extreme drought years, shallow geometric depressions might dry out differently, creating subtle spectral contrasts in Sentinel-2 images. Another trick: look at vegetation indices or false-color composites for patterns – ancient causeways or mounds can alter soil nutrients and moisture, affecting vegetation height or type (sometimes an elevated ancient causeway supports a different tree species line or stunted growth). If you suspect a spot based on the ML model or environmental criteria, you can task high-res imagery (if time/contest allows) or see if any SAR coherence change (from dry to wet season) highlights a linear feature there.
Automated anomaly detection: Employ algorithms to flag potential anthropogenic patterns. For example, generate a “local relief model” by removing large-scale topography (a technique used in LiDAR archaeology
cdn.openai.com
) – this can make small earthworks pop out. Similarly, cluster analysis on spectral data might separate “uniform grid-like clearings” from natural forest gaps. Any computational approach that yields candidates is useful to then inspect manually.
Integration of Clues: A powerful discovery often comes from multiple independent pieces of evidence pointing to the same location. Strive to integrate different data sources for any candidate site:
Did your ML model highlight a particular 10×10 km area as high probability? Check if that area has any weird aerial features or known indigenous history.
Did you find a curious shape on satellite images? See if that location also fits the environmental pattern (near other known sites or on a certain soil) and if any historical account mentions a “village” at that river mile.
For example, you might notice your model flags an area on an interfluve between rivers. Upon scanning, you spot a faint square outline on a soybean field there. You then search expeditions and find a 1920s diary noting “ancient mounds” in that vicinity. These layered clues build a convincing case. In the sample rationale given by organizers, a winning submission combined LiDAR evidence (a concentric 120 m ditch with a platform), a GEDI canopy anomaly, a 1920 diary waypoint near the spot, and a Sentinel-2 soil scar – all aligning within a few hundred meters
cdn.openai.com
cdn.openai.com
. Reaching this level of multi-source confirmation should be the goal.
Validation & Novelty Check: For each promising site candidate you identify, validate it rigorously:
Cross-check coordinates against databases of known sites (to ensure it’s not already documented)
cdn.openai.com
. For geoglyphs, you might use the published lists of hundreds of known geoglyphs
jqjacobs.net
. If you find a match, drop it (not novel). If not, confidence grows that it’s new.
If possible, share the coordinate (privately) with an expert or use logic: is it on protected land (less explored) or an area archaeologists haven’t surveyed? Does it physically make sense as a site (size, shape, context)? Eliminate false positives (e.g., an agricultural field pattern or a natural oxbow lake shape) by checking multiple imagery dates and perhaps even using an LLM to describe the image (more on that soon).
Document everything needed to reproduce the find: which data layers, what processing, and how you noticed it. This documentation helps with both reproducibility and the final explanation.
Example Workflow in Practice: To illustrate, a baseline discovery process might look like: “Using a Random Forest model trained on known site locations and 65 environmental variables, I generated a heatmap of high-probability zones across the Amazon
peerj.com
. One hotspot was in northern Bolivia near a tributary of the Iténez River. I obtained a 12.5 m DEM for that area and applied a local relief filter, revealing a circular depression ~100m across. Overlaying Sentinel-2 imagery showed a ring of lighter vegetation matching the depression. Searching the 1910 Rondon expedition diary (via Library of Congress), I found mention of a ‘ancient circular embankment’ near a “bend in the Verde River” – aligning with this location. No known site is listed there in published catalogs. Therefore, I propose this as a previously unreported ring-ditch earthwork site.” This hypothetical example demonstrates how multiple lines of open-source evidence can be woven together to identify and justify a new site discovery.
2.3 Tools for Reproducible Analysis
Being solo doesn’t mean doing everything manually. Leverage open-source tools and libraries for efficiency and reproducibility:
GIS Software: QGIS (desktop GIS) is invaluable for visualizing layers, drawing regions of interest, and running basic spatial analysis (buffering, clipping, etc.). It’s free and has plugins for things like LiDAR processing (e.g., LAStools plugin for LiDAR). You can compose your datasets in QGIS and export nice maps for your report.
Programming Libraries: Python with libraries such as rasterio/rioxarray (to read/write geotiffs), geopandas/shapely (for vector data), scikit-learn or xgboost (for ML modeling), and matplotlib/seaborn (for plotting). Jupyter notebooks are great for documenting the steps. R is also an option (with similar spatial packages like raster, sf, and machine learning libs).
Google Earth Engine (GEE): If coding locally is a challenge for large data, GEE allows you to query and process satellite data at scale using JavaScript or Python API. For example, you could use GEE to pull all Sentinel-2 images over a region and composite them to the best quality, or to calculate NDVI anomalies during drought periods – all without downloading hundreds of GB of data.
Cloud or Colab: If your computations (like training an ML model or processing gigabytes of images) are heavy, consider using Google Colab or Kaggle Notebooks (which often provide free GPU/TPU and decent memory). This also helps encapsulate your code and results in one place, which you can share for reproducibility.
Version Control: Use Git to track changes in your code or analysis. It’s good practice to have a public GitHub repository (if allowed by the competition rules) or at least a private one to which judges can be given access. This repository would contain your data processing scripts (excluding any huge raw data files) and perhaps sample data outputs, along with instructions to run or a Docker environment if applicable.
Visualization Tools: In presenting archaeological findings, visuals are key. Tools like QGIS or matplotlib can make maps highlighting your site. You might also use simple 3D terrain viewers (even Google Earth’s 3D view or Blender with DEMs) to create perspective images of the terrain with your site marked, to illustrate it clearly.
Workflow Logging: Keep a detailed log (even just a text file or a lab notebook) of your steps, data sources (with URLs, dates accessed), parameter settings for models, and even the prompts you use with any AI. This ensures you can later cite exactly how you got a result. It’s also something judges will appreciate – it shows scientific rigor and makes reproduction feasible
cdn.openai.com
cdn.openai.com
.
By combining these freely available data sources and tools, a solo participant can assemble a workflow comparable to professional remote sensing archaeology efforts. The key is to iterate between model-driven prediction and data-driven detection, cross-validating one with the other. The next section will explore how to augment each stage of this pipeline with the power of LLMs, to further boost your efficiency and insight.
3. Incorporating Large Language Models (LLMs) at Each Stage of Discovery
Large Language Models like GPT-4 can be more than just fancy text generators – they can act as research assistants throughout your project. Here we detail how LLMs (such as OpenAI’s GPT-4.1 or smaller open models like the “o3”/“o4-mini” referenced in the challenge) can be woven into each stage of the archaeological site discovery process. The emphasis is on using LLMs to augment your capabilities in understanding texts, finding patterns, and even analyzing data, rather than replacing your own critical thinking. Remember to always verify LLM outputs, especially for critical decisions.
3.1 Literature Review & Historical Data Mining
Using LLMs for background research: Start by feeding the LLM relevant literature or asking it pointed questions about Amazonian archaeology. For example, you can prompt GPT-4 with: “Summarize the main strategies used in remote sensing archaeology in Amazonia.” If given the content of key papers, the LLM can highlight important methods (e.g., LiDAR reveals urban sites; geoglyphs found by satellite; environmental predictors for settlements, etc.). This can quickly bring you up to speed on domain knowledge, given you are not an archaeology expert. Summarizing colonial and expedition literature: One powerful use of LLMs is digesting long, old texts. Suppose you have a 200-page XIX-century exploration narrative (filled with archaic language) — an LLM can extract the useful bits. You might prompt: “Read this expedition diary text and extract every sentence that mentions a river, compass direction, or distance traveled.” to pull out geographic clues
cdn.openai.com
. Or more generally: “Summarize any references to ancient ruins or earth mounds in this text.” The model can act like a smart filter, saving you hours of skimming. Organizers even suggested using an LLM to scan diaries for mentions of rivers and distances, which you can then geocode into approximate coordinates
cdn.openai.com
. This could lead you to a specific location described 100 years ago that corresponds to a potential site today. Language translation and interpretation: Many useful documents might be in Portuguese or Spanish (e.g., Brazilian archaeological reports, or chronicles of Spanish conquistadors). If you’re not fluent, LLMs can translate or summarize these. A prompt like: “Translate this Portuguese article about Acre geoglyphs and summarize any details about how they were discovered.” would yield accessible information. GPT-4’s ability to handle multiple languages and provide context will help you avoid missing non-English insights. Knowledge extraction for patterns: You can feed the LLM curated facts about known sites (perhaps from your database: e.g., location, site type, description) and ask it to identify commonalities. “Here are brief summaries of 20 known Amazonian archaeological sites. What patterns do you see in terms of location and environment?” The LLM might respond with something like, “Most sites are near water and on fertile soil; none are in upland terra firme forest far from rivers,” which aligns with expert knowledge and can reinforce your search criteria.
3.2 Planning & Hypothesis Generation
Brainstorming with the LLM: Use the model as a sounding board. For example: “Given that geoglyphs in Acre tend to appear on flat plateaus, where else in the Amazon might I find similar plateaus to search?” A well-informed LLM (with either training knowledge or augmented by you providing context) could suggest areas by name or by description. It may recall, for instance, that “the Llanos de Mojos in Bolivia also have flat savanna areas where earthworks have been found”
smithsonianmag.com
, prompting you to include that region if you hadn’t considered it. LLM-assisted environmental reasoning: If you have output from your ML model or environmental analysis, you can let the LLM help interpret it. For instance, after generating your predictive map, you might give the LLM a simplified version: “We have high probability for undiscovered sites in regions A, B, and C, which have these environmental profiles. Based on known pre-Columbian cultures, which of these regions seem most plausible for complex settlements and why?” The model could discuss historical context (maybe it “knows” that region A was home to the Marajoara culture, etc.) and give you a qualitative sense-check or fresh ideas linking data to real-world context. Guiding satellite image targeting: This is a more creative use – you could have the LLM assist in focusing your imagery analysis. For example, you might describe a sub-region’s characteristics and ask: “What kinds of anthropogenic patterns should I look for in satellite images of this region?” If it’s forested, it might say “Look for straight lines or rectilinear edges in canopy texture (indicating ancient causeways or ditches)” or if it’s savanna, “look for circular discolorations in grass.” While you likely know these things, an LLM can help ensure you don’t overlook a possibility (like “check along natural lake shores for symmetrical mounds, as some cultures built mounds near water”).
3.3 Data Processing & Analysis with LLMs
Assisting in remote sensing analysis: Traditionally, image analysis is done with specialized software, but interestingly, LLMs (especially if multimodal or used in creative ways) can assist:
The challenge hints at a prompt: “Scan this LiDAR raster for geometric shapes (rectangles, circles, straight ditches). Return rough center coordinates for anything ≥ ~80 m across.”
cdn.openai.com
. This suggests using an LLM to identify features from data. How might one do this? One approach could be converting raster data into a simplified text matrix or descriptors that a text-only model can parse. Alternatively, with a multimodal model (if GPT-4 vision or other tools are available), you might actually feed an elevation image or its ASCII representation for analysis. The model could output coordinates of anomalies. For instance, you give it a downsampled grid like:
python-repl
Copy
Edit
0 0 1 5 2 0 ...  
0 1 7 10 7 1 ...  (some representation of elevation differences)
...
and prompt it to find any pattern of a ring or rectangle. This is experimental, but small GPT-4 powered tools might detect a simple geometric shape in a matrix of numbers.
For Sentinel-2 images, you likely can’t feed raw pixel data to a text LLM, but you can preprocess (maybe classify the image into “green vegetation, bare soil, water, etc.” and then describe it to the LLM). The provided example prompt is: “Given a coordinate and the matching Sentinel-2 scene, tell me in plain English whether the surface patterns look man-made or natural, and include a 0–1 confidence.”
cdn.openai.com
. To use this, you could supply the LLM with a description like: “At these coordinates, the satellite image shows a light green rectangular patch in a dark green forest matrix, with straight edges on two sides.” The LLM might respond: “It looks man-made (cleared area with straight boundaries) with 0.9 confidence,” or “It appears natural (irregular shape of a river oxbow) with 0.2 confidence.” Essentially, the LLM can serve as an interpreter of what certain visual cues might mean, especially if you convert them to descriptive text first. This helps you quickly triage sites: a high confidence “man-made” pattern is worth more attention than one the LLM finds likely natural.
Automating text extraction: Beyond imagery, any repetitive text processing can be handed to the LLM. For example, if you downloaded 50 PDFs of archaeological studies, you could ask the LLM to extract all coordinates mentioned in them or all instances of keywords like “causeway” or “ditch”. It can compile these references, which you can then verify. This can reveal, say, that a 1940 journal article noted a mound at a certain location that was never followed up on – gold nuggets for your search. Pattern recognition in tabular data: Suppose you created a table of environmental variables for both known site locations and random locations. You could ask the LLM to analyze this table (in a CSV to text format) and find distinguishing features. It might output insights like “Known sites have on average 1 km distance to water vs 5 km for random points,” which you can then fact-check. While statistical analysis is better done with code, the LLM can provide a human-like summary of data patterns which might inspire how you weight certain factors.
3.4 Report Writing and Explanation
Using LLMs for documentation: After you’ve done the hard work, an LLM can help you articulate it clearly. You can draft sections of your final report and have the LLM polish the language or check for completeness. For instance: “Here is my explanation for why I think this site is man-made. Can you suggest improvements or point out if I missed explaining any evidence?” GPT-4 can refine your writing, ensure you define terms that a general audience may not know (like explaining terra preta or LiDAR the first time you mention them), and even help format it in the required style (e.g., it can help generate citation placeholders if you feed it references). Presentation and storytelling: If part of the competition is a video or story, LLMs are great for scripting. You might use it to create a compelling narrative of your discovery: “Draft a dramatic but factual narrative of how an ancient city was discovered in the Amazon, to use in my presentation.” This can give a flavorful storyline which you can then adapt. It ensures you communicate not just dry facts but also the significance and excitement of the discovery, which can engage judges. QA and sanity checks: Before submitting, you can leverage the LLM as a pseudo-reviewer. Provide it with your summary and ask it to critique: “As a judge, what questions or doubts might you have about this submission?” or “Is there any step in my method that seems unclear or unsupported?” The LLM might point out, for example, “You claim this is a new site, but did you check the national registry? That could be a question.” You can then make sure to address that in your report (e.g., “We cross-checked against known site databases
cdn.openai.com
 to ensure novelty”). In essence, LLMs act as accelerators for your thinking and writing. They can summarize complex info, suggest connections, and even handle some analytical chores – allowing you to focus on high-level reasoning and creative problem solving. Just remember that LLM outputs should be verified and not used blindly. Where possible, use the LLM in a closed-loop with your data: Retrieval-Augmented Generation (RAG) is ideal, which we discuss next in the context of tools like LangChain.
4. Tools and Pipelines for an LLM-Powered (RAG) Workflow
Integrating LLMs into your project is made easier by specialized frameworks. In particular, Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to give LLMs up-to-date, relevant information
md-hadi.medium.com
md-hadi.medium.com
. RAG frameworks combine a retriever (fetching data from your documents or database) with a generator (the LLM that produces answers or analysis using that data)
md-hadi.medium.com
. Two notable tools to implement RAG and complex LLM workflows are LangChain and LangGraph.
LangChain: This is a popular Python framework that streamlines the creation of LLM-driven applications. It allows you to chain together LLM calls with other utilities like database queries, API calls, or custom functions. For example, you can create a chain that: (1) takes a question, (2) uses a vector database to retrieve relevant text (like passages from expedition diaries or research papers you indexed), and (3) feeds both question and text to GPT-4 to get a context-aware answer. LangChain provides abstractions for these steps, making it easier to implement RAG rather than coding from scratch. It’s very flexible – you can build a Q&A bot over your archaeology PDFs or an agent that, say, given a location, will automatically retrieve any descriptive text about that location from your notes and have the LLM summarize it.
LangGraph: While LangChain focuses on linear chains of tasks, LangGraph introduces a graph-based approach to orchestrate complex LLM workflows
md-hadi.medium.com
md-hadi.medium.com
. It’s useful when you have branching or iterative processes. In an archaeological discovery context, you might design a LangGraph workflow like: Step A: If satellite image available -> describe image -> feed to LLM for analysis; Step B: Query vector DB of texts for that location -> summarize via LLM; Step C: Combine results -> decision node (LLM decides if evidence is strong or if more data is needed) -> if more needed, go back and maybe trigger a different tool (e.g., search for climate data); Step D: generate final hypothesis text. LangGraph can manage this as a directed graph of states, where the LLM can even loop or iterate until a condition is met
md-hadi.medium.com
. This is advanced usage, but the benefit is orchestrating multiple data sources and LLM calls in a coherent pipeline. Both LangChain and LangGraph are LLM orchestration frameworks that have become popular by 2025 because they simplify building agentic AI systems and ensure reproducibility of multi-step AI workflows
md-hadi.medium.com
md-hadi.medium.com
.
Vector Databases (for RAG): Tools like Chroma, FAISS, or Pinecone often complement the above frameworks. You would embed your text data (e.g., convert all your PDFs and notes into vector embeddings using an embedding model) and store them. Then given a query (like “clues of ancient sites near X river”), the retriever finds the closest text chunks, which are then passed to the LLM. This ensures the LLM’s output is grounded in real data you have, mitigating hallucinations and increasing accuracy
md-hadi.medium.com
md-hadi.medium.com
. In practice, you could load all known site descriptions, all your compiled environmental stats, and even your own intermediate results into a vector store – allowing an LLM agent to dynamically pull facts as needed during analysis.
Examples of AI pipelines for site prediction: While the exact application (archaeological site prediction) is niche, it’s akin to other RAG use cases. For instance, you might build an agent that, given a candidate location, will automatically:
Retrieve any known data about that location (from your databases: e.g., nearest known site, environmental variables, any mention in text).
Call an LLM to assess: “Given all this, is there evidence this location harbors an unknown archaeological site? Answer with reasoning.” This is essentially a validation agent using RAG.
If the confidence is low, the agent could then decide to try another location or request additional info (like a larger radius search in the retriever). LangChain supports this kind of tool-using agent via its Agent classes, and LangGraph can add more structured control (for example, ensuring the agent tries a fixed sequence of retrievals).
Another pipeline could cluster potential site locations: you could have the LLM embed the textual descriptions of each candidate site (like “circular ditch in clay soil near Purus River”) and use a clustering algorithm on embeddings to see groupings – perhaps separating “geoglyph-like” candidates from “mound village” candidates. This might inform different validation strategies for each cluster. While a traditional ML approach could do this, an LLM might classify them by type quickly if given the descriptions.
Other Tools: It’s worth noting other frameworks mentioned in AI circles: LlamaIndex (GPT Index) is another library that simplifies RAG (especially document Q&A), and Haystack is an open-source alternative focused on search + LLM pipelines. However, LangChain and LangGraph were explicitly suggested, likely because LangChain is widely used and LangGraph is a newer approach gaining traction
md-hadi.medium.com
md-hadi.medium.com
. These tools are both beginner-friendly (with lots of examples) and powerful enough for complex tasks
md-hadi.medium.com
. They allow you to keep your LLM usage organized and traceable – important for debugging and trustworthiness (LangChain even has evaluation modules to help you gauge LLM output quality).
In summary, consider setting up a small RAG pipeline for your project:
Use LangChain to connect your data (texts, coordinates, images via descriptions) with the LLM, so you can ask detailed questions and get evidence-backed answers.
Use LangGraph if you need a more controlled multi-step agent (for example, an agent that tries multiple prompts or tools in sequence to decide on site viability).
Incorporate a vector store to supply the LLM with real references (you could even have it cite those references in your final report, similar to how this answer is providing citations).
This kind of AI-assisted pipeline not only boosts your analytical capabilities but also demonstrates novelty in approach – few archaeological projects have used full AI agent workflows. Just be sure to clearly document how the LLM is used (prompts, data sources fed, etc.) so it’s transparent to judges, and evaluate its output carefully.
5. Tips for Clarity, Reproducibility, Novelty, and Presentation
Finally, let’s cover some best practices to ensure your work stands out in the competition and satisfies the criteria of evidence, reproducibility, and novelty. These tips will help refine both your process and how you present your findings:
Clarity in Communication: Write your reports or notebooks as if explaining to someone from a different background. Define technical terms (e.g., “LiDAR – a laser-scanning technique that maps ground elevation through forest canopy”) and avoid unnecessary jargon. Use clear headings and subheadings to organize the content (e.g., Data Collection, Analysis Methodology, Results, Conclusion). Keep paragraphs concise and use bullet points or numbered lists for step-by-step sections (much like this guide) to improve readability. A judge or reader should be able to skim and grasp your approach quickly. Before submission, consider having an LLM or a colleague read your report to flag any confusing parts – clarity is king.
Reproducibility: This is crucial in a scientific competition. Provide exact references for all data sources (with links if possible) and describe any preprocessing in detail. If you used a specific dataset (say, WorldClim v2 temperature), cite it properly
peerj.com
. If you wrote code, include it or at least pseudocode/flowcharts in an appendix. Logging your steps pays off here: for every map or model output, state what inputs produced it (e.g., “Random Forest model trained on 70% of 1,100 known sites, with these hyperparameters…” or “coordinates converted to WGS84 datum before overlay”). Ideally, package your analysis in a Jupyter Notebook or similar, with markdown explanations and code outputs interleaved. This way, judges can literally rerun or trace your analysis. Tip: If the competition allows, upload a subset of data and your code on a GitHub repo or as a Kaggle Notebook for easy access. Reproducibility also means using consistent naming and logging – for instance, label your figures and file names clearly (Figure 1 corresponds to a specific step). These practices show you approached the task rigorously like a scientific study, which will score points.
Evidence and Referencing: Whenever you make a claim or choose a strategy, back it up with evidence or literature. For example, if you say “sites tend to be near water,” cite a source or your own data analysis
jqjacobs.net
. By including academic references (as we have throughout this guide) you demonstrate that your approach is grounded in existing knowledge. If you incorporate LLM analysis, treat its outputs as hypotheses that you then verify – and document that verification. For instance, “The LLM suggested this clearing might be man-made with 90% confidence
cdn.openai.com
, and upon checking multi-date imagery, we indeed saw the outline persist, reinforcing that conclusion.” Use screenshots or small excerpts of data as evidence when appropriate (while respecting any sensitive info rules). The goal is to convince judges that every major step or discovery you present is supported by objective evidence (image, statistic, reference, etc.), not just speculation.
Ensuring Novelty: Emphasize how your identified sites or insights are new. As mentioned, cross-check against known site lists and state that you’ve done so to avoid rediscoveries
cdn.openai.com
. If your approach itself is novel (like using an LLM agent or a new combination of data sources), highlight that in the methodology section. Judges often reward innovative methods, especially if they’re effective. However, novelty should not come at the expense of credibility – a wild theory with no evidence won’t score well. Balance creativity with substantiation. One tip: explicitly include a short section in your report titled “Novelty and Contribution” where you list what’s new (e.g., “First application of a GPT-4-driven analysis pipeline in Amazon archaeology”, “Integrated GEDI LiDAR data which hasn’t been used in prior site predictions”, or “Discovered X new candidate sites not recorded in literature”). This signals to judges exactly what you’re contributing beyond prior work.
Visualization and Presentation Quality: A picture is worth a thousand words – especially in conveying geographic findings. Make sure to include clear maps of your study areas and site locations. Use annotations to mark the suspected sites, perhaps with different symbols for different evidence types (triangle for LiDAR detection, square for model prediction, etc.). Visualize supporting data too: maybe a chart showing how your model ranked various regions, or a diagram of your LLM-powered workflow (if it can be simplified). When embedding images, ensure they are high enough resolution and have captions. For example, if you show a satellite image of a geoglyph, the caption might read: “Satellite image of a square geoglyph enclosure (200 m across) revealed after deforestation in Acre, Brazil

.” This way even someone skimming sees the point. In text, reference your figures (e.g., “(see Figure 2)” or using the citation as above).
Additionally, if the competition allows a video or interactive element, take advantage of that. A short video flying over the LiDAR 3D model of your discovered site or a before/after overlay of imagery can be compelling. It also underscores your thoroughness. Just be sure to also describe everything in writing for completeness.
Prompt Engineering and LLM Usage Notes: If you leaned on LLMs for parts of the project, include a brief note on how you did it. This could be in an appendix or footnotes. For instance, list example prompts that were particularly effective (the starter pack actually expects participants to share prompt versions/logs)
cdn.openai.com
. This transparency shows you didn’t just “consult a magic oracle” – you systematically integrated AI. It might also help others replicate or learn from your approach. However, be cautious not to oversell LLM outputs as facts; always couch them as assistance or analysis. By showing the exact prompts and any human verification step, you maintain credibility.
Don’t Over-tune or Over-complicate: A practical tip from organizers: “Don’t over-tune prompts: start broad, then add constraints only when noise overwhelms the signal.”
cdn.openai.com
. Similarly, don’t overfit your models or obsess on a single hypothesis if evidence is thin. Keep an open mind and a breadth-first search approach until you have strong candidates, then deepen the analysis on those. This philosophy, if evident in your report, will show you approached the problem methodically. For example, you might mention how you first cast a wide net (broad model over Amazon), then focused on top 5 areas, then progressively narrowed to 1-2 best site candidates as multiple data sources confirmed them. This narrative demonstrates a sound scientific process.
Enthusiasm and Storytelling: Lastly, while maintaining academic rigor, let your excitement for discovery come through. The competition is about finding “legends” in real data – if you truly believe you found something significant, convey that. You can record a short clip or even include a personal note of why it’s exciting (“When the elevation model revealed a perfect circle, I could hardly believe my eyes…”). This human element can make your submission more memorable. Judges are people – a bit of genuine enthusiasm, backed by evidence, can leave a lasting impression. Just avoid unsupported hyperbole; phrase your excitement around the evidence (“– a moment of discovery captured in the LiDAR image above –”).
By following these tips, you’ll produce a submission that is clear in thought, rich in evidence, reproducible in method, novel in contribution, and compelling in presentation. In a competition aiming to push the boundaries of how we discover the past, combining solid scientific techniques with modern AI assistance – and communicating it effectively – will put you in an excellent position.
Conclusion
Unearthing unseen archaeological sites in the Amazon from behind a computer screen is no small feat – but with the right strategy, data, and tools, it’s an achievable and thrilling endeavor. To score highly, approach the task like a detective: use every clue from remote sensing tech to centuries-old journals, leverage AI to amplify your detective work, and cross-verify everything to build a convincing case. We outlined how to do this step-by-step, provided a baseline methodology with open-source data that anyone can follow, and showed how modern LLMs and RAG pipelines can act as force-multipliers in your research. By adhering to principles of clarity, reproducibility, and innovation, you won’t just find new sites – you’ll be able to convince others you found them and explain how. Good luck with your archaeological adventure in the Amazon, and may your discoveries help illuminate the rich, hidden history of this “green ocean” of a rainforest!
